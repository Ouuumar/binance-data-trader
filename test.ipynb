{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_hist_klines(conn, symbol, table, client_interval, client): \n",
    "    \"\"\" Return the symbol's historical klines data from Binance\n",
    "    Parameters \n",
    "    ---------------------------------------\n",
    "        con : Engine()\n",
    "                the engine/connection of MySQL\n",
    "        symbol : str\n",
    "                symbol (pair of crypto)\n",
    "        client_interval : str\n",
    "                interval of klines\n",
    "    Return\n",
    "    ---------------------------------------\n",
    "            client.get_historical_klines() : json\n",
    "                The json symbol data klines\n",
    "    \"\"\"\n",
    "    # Check if no symbol's data in table, download data from the oldest date available e.g 1 Aug 2017 (Binance founded date)\n",
    "    if ((conn.execute(text(f\"SELECT COUNT(*) FROM {table} WHERE symbol = '{symbol}'\")).scalar() == 0)):\n",
    "        logging.info(f\"No data for {symbol}, currently getting {symbol} data from 1 Aug 2017 ...\")\n",
    "        return client.get_historical_klines(symbol, client_interval, \"1 Aug 2017\")\n",
    "    else :\n",
    "        # Get the most recent date of the data if the table is not empty in order to download from the most recent date\n",
    "        most_recent_date_in_db = (conn.execute(text(f\"SELECT max(open_time) FROM {table} WHERE\\\n",
    "        symbol = '{symbol}'\"))).scalar()\n",
    "        one_hour_from_db_date= str(datetime.now() + timedelta(hours=1))[:19] # Select only %y-%m-%d %H:%M:%S and add 1 hour\n",
    "        one_hour_from_db_date = parser.parse(one_hour_from_db_date) # Parse the date\n",
    "        logging.info(f\"{symbol} data already present, getting {symbol} data from {most_recent_date_in_db} + 1 hour if exists\")\n",
    "        return client.get_historical_klines(symbol, client_interval, str(one_hour_from_db_date))\n",
    "\n",
    "def create_con(user, pw, ip, port, db):\n",
    "        \"\"\" Return the engine (the connection) to interact with MySQL\n",
    "        Parameters \n",
    "        ---------------------------------------\n",
    "                user : str\n",
    "                        the user name\n",
    "                pw : str\n",
    "                        the user password\n",
    "                db : str\n",
    "                        the database name\n",
    "        Return\n",
    "        ---------------------------------------\n",
    "                engine : Engine(mysql+pymysql://{user}:{pw}@localhost)\n",
    "        \"\"\"\n",
    "        engine = create_engine(f\"mysql+pymysql://{user}:{pw}@{ip}:{port}/{db}\")\n",
    "        logging.info(f\"Connection at {engine} : created !\")\n",
    "        return engine\n",
    "\n",
    "def export_data(conn, data, schema, table):\n",
    "        \"\"\" Load data into MySQL\n",
    "        Parameters \n",
    "        ---------------------------------------\n",
    "                con : Engine()\n",
    "                        the engine/connection of MySQ\n",
    "                data : pandas.DataFrame()\n",
    "                        symbols data\n",
    "                table : str\n",
    "                        the table name\n",
    "        Return\n",
    "        ---------------------------------------\n",
    "                Nothing\n",
    "        \"\"\"\n",
    "        data.to_sql(con=conn, schema=schema, name=table, if_exists=\"append\")\n",
    "        \n",
    "\n",
    "def process_hist_data(data, symbol):\n",
    "    \"\"\" Return DataFrame of the data processed \n",
    "    Parameters \n",
    "    ---------------------------------------\n",
    "            data : json\n",
    "                    symbol data\n",
    "            symbol : str\n",
    "                    symbol (pair of crypto)\n",
    "    Return\n",
    "    ---------------------------------------\n",
    "            df : pandas.Dataframe()\n",
    "                symbol data processed into DataFrame\n",
    "    \"\"\"\n",
    "    # Return processed json klines data into pandas dataframe\n",
    "    df = pd.DataFrame(data, columns=[\"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\",\"close_time\",\\\n",
    "    \"quote_asset_volume\", \"number_of_trades\", \"taker_buy_base_asset_volume\",\"taker_buy_quote_asset_volume\",\"ignore\"])\n",
    "    \n",
    "    df.drop(\"ignore\",axis=1, inplace=True)\n",
    "    df['open_time'] = pd.to_datetime(df['open_time']/1000, unit='s')\n",
    "    df['close_time'] = pd.to_datetime(df['close_time']/1000, unit='s')\n",
    "\n",
    "    numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'quote_asset_volume', 'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume']\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, axis=1)\n",
    "    \n",
    "    df.set_index(\"open_time\", inplace=True)\n",
    "\n",
    "    df[\"symbol\"] = symbol\n",
    "    df = df.astype({\"symbol\" : \"string\"})\n",
    "    return df\n",
    "\n",
    "def etl(client, symbols=[\"ETHUSDT\"]):\n",
    "    \"\"\" Extract, transform and load the symbol's klines data processed\n",
    "    Parameters \n",
    "    ---------------------------------------\n",
    "        symbols : list\n",
    "                the list of symbol to treat\n",
    "        client : Binance Client\n",
    "    Return\n",
    "    ---------------------------------------\n",
    "        Nothing\n",
    "    \"\"\"\n",
    "    conn = create_con(user=os.environ[\"MYSQL_USER\"], pw=os.environ[\"MYSQL_PASSWORD\"], ip=os.environ[\"MYSQL_IP\"],port=os.environ[\"MYSQL_PORT\"], db=os.environ[\"MYSQL_DATABASE\"])\n",
    "    if symbols == None:\n",
    "        logging.info(\"No symbols\")\n",
    "        symbols = conn.execute(text(\"SELECT DISTINCT symbol FROM historical_klines\")).fetchall()\n",
    "    for crypto in symbols:  \n",
    "        historical_data = get_hist_klines(conn, str(crypto), os.environ[\"KLINES_TABLE\"], client.KLINE_INTERVAL_1DAY, client=client)\n",
    "        logging.info(f\"{crypto} downloaded\")\n",
    "        df = process_hist_data(historical_data, crypto)\n",
    "        logging.info(f\"{crypto} processed\")\n",
    "        export_data(conn=conn, data=df, schema=os.environ[\"MYSQL_DATABASE\"], table=os.environ[\"KLINES_TABLE\"])\n",
    "        logging.info(f\"{crypto} pushed to database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = create_con(user=os.environ[\"MYSQL_USER\"], pw=os.environ[\"MYSQL_PASSWORD\"], ip=\"localhost\",port=3307, db=os.environ[\"MYSQL_DATABASE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"SELECT * FROM historical_klines WHERE symbol='ETHUSDT'\", con=conn)\n",
    "df = df.set_index(\"close_time\")\n",
    "df.index = pd.to_datetime(df.index)\n",
    "df = df.drop([\"symbol\", \"number_of_trades\", \"open_time\"], axis=1)\n",
    "df = df.sort_index()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"close\"]\n",
    "train_data = data.sample(frac=0.8,random_state=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data.drop(train_data.index)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = train_data\n",
    "ts_logtransformed = np.log(ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposition = seasonal_decompose(ts_logtransformed, period=1) #, \n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomposed_TS = residual\n",
    "decomposed_TS.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_logtransformed, order=(8, 1, 0))  \n",
    "results_AR = model.fit()  \n",
    "RSS = results_AR.fittedvalues-ts_diff_logtrans\n",
    "RSS.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_logtransformed, order=(2, 1, 0))  \n",
    "results_AR = model.fit()  \n",
    "RSS = results_AR.fittedvalues-ts_diff_logtrans\n",
    "RSS.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_AR.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_logtransformed, order=(0, 1,18)) \n",
    "results_MA = model.fit()  \n",
    "RSS = results_MA.fittedvalues-ts_diff_logtrans\n",
    "RSS.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_MA.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARIMA(ts_logtransformed, order=(8, 1, 18))  \n",
    "results_ARIMA = model.fit()  \n",
    "\n",
    "RSS =results_ARIMA.fittedvalues-ts_diff_logtrans\n",
    "RSS.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_ARIMA.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff = pd.Series(results_ARIMA.fittedvalues, copy=True)\n",
    "print(predictions_ARIMA_diff.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_diff_cumsum = predictions_ARIMA_diff.cumsum()\n",
    "print(predictions_ARIMA_diff_cumsum.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA_log = pd.Series(ts_logtransformed.iloc[0], index=ts_logtransformed.index)\n",
    "predictions_ARIMA_log = predictions_ARIMA_log.add(predictions_ARIMA_diff_cumsum,fill_value=0)\n",
    "predictions_ARIMA_log.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_ARIMA = np.exp(predictions_ARIMA_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Timestamp('2023-10-24 00:00:00')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tomorow = [pd.Timestamp(datetime.date.today() + datetime.timedelta(days=1))]\n",
    "tomorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tomorow = [pd.Timestamp(datetime.date.today() + datetime.timedelta(days=1))]\n",
    "tomorow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [pd.Timestamp('2023-10-23'), pd.Timestamp('2023-10-24'), pd.Timestamp('2023-10-25'),pd.Timestamp('2023-10-26'), pd.Timestamp('2023-10-27'), pd.Timestamp('2023-10-28'), pd.Timestamp('2023-10-29')]\n",
    "tomorow = [pd.Timestamp(datetime.date.today() + datetime.timedelta(days=1))]\n",
    "\n",
    "forecast = pd.Series(data=results_ARIMA.forecast(steps=1).to_list(), index=tomorow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_ARIMA.save(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARIMAResults\n",
    "import pickle \n",
    "\n",
    "model = pickle.load(open(\"/home/omar/DevWSL/binance-data-trader/airflow/models/2023-10-23_arima_model.pkl\", 'rb'))\n",
    "# model = ARIMAResults.load(\"../airflow/models/2023-10-23_arima_model.pkl\")\n",
    "tomorow = [pd.Timestamp(datetime.date.today() + datetime.timedelta(days=1))]\n",
    "forecast = pd.Series(data=model.forecast(steps=1).to_list(), index=tomorow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import logging\n",
    "import pandas as pd\n",
    "from dateutil import parser\n",
    "from datetime import datetime, timedelta\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 11, 4, 21, 14, 14, 530037)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = datetime.now() + timedelta(hours=1)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
